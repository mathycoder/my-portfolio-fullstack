{"ast":null,"code":"'use strict';\n\nvar logger = require('../../logger').child({\n  component: 'tx_segment_normalizer'\n});\n\nmodule.exports = TxSegmentNormalizer;\n\nfunction TxSegmentNormalizer() {\n  this.terms = [];\n}\n/**\n * This normalize method is wicked. The best bet is to read the spec:\n * https://newrelic.atlassian.net/wiki/pages/viewpage.action?spaceKey=eng&title=Language+agent+transaction+segment+terms+rules\n *\n * A copy paste of the rules that were followed:\n *  1. Find the first rule where the prefix key matches the prefix of the\n *     transaction name. If no matching rules are found, abort.\n *  2. Strip the prefix from the transaction name.\n *  3. Split the rest of the transaction name into segments on slashes ('/').\n *  4. For each segment:\n *      If the segment appears in the array of strings given under the terms key,\n *      keep it unchanged. Else, replace it with a placeholder ('*')\n *  5. Collapse all adjacent placeholder segments into a single '*' segment.\n *  6. Join together the modified segments with slashes, and re-prepend the prefix.\n *\n * @param {string} path - The transaction metric path to normalize.\n *\n * @return {NormalizationResults} - The results of normalizing the given path.\n */\n\n\nTxSegmentNormalizer.prototype.normalize = function normalize(path) {\n  var currentTerm;\n  var prefix;\n\n  for (var i = 0; i < this.terms.length; i++) {\n    currentTerm = this.terms[i];\n    prefix = currentTerm.prefix;\n\n    if (path.lastIndexOf(prefix, 0) === -1) {\n      continue;\n    }\n\n    var fragment = path.slice(prefix.length);\n    var parts = fragment.split('/');\n    var result = [];\n    var prev;\n    var segment;\n\n    for (var j = 0; j < parts.length; j++) {\n      segment = parts[j];\n      if (segment === '' && j + 1 === parts.length) break;\n\n      if (currentTerm.terms.indexOf(segment) === -1) {\n        if (prev === '*') continue;\n        result.push(prev = '*');\n      } else {\n        result.push(prev = segment);\n      }\n    }\n\n    logger.trace('Normalizing %s because of rule: %s', path, currentTerm);\n    return {\n      matched: true,\n      // To match MetricNormalizer\n      ignore: false,\n      // ^^\n      value: prefix + result.join('/')\n    };\n  }\n\n  return {\n    matched: false,\n    // To match MetricNormalizer\n    ignore: false,\n    // ^^\n    value: path\n  };\n};\n\nTxSegmentNormalizer.prototype.load = function load(json) {\n  if (Array.isArray(json)) {\n    this.terms = filterRules(json);\n  } else {\n    logger.warn('transaction_segment_terms was not an array got: %s (%s)', typeof json, json);\n  }\n};\n\nfunction filterRules(rules) {\n  var map = Object.create(null);\n\n  for (var i = 0, l = rules.length; i < l; ++i) {\n    var prefix = rules[i].prefix;\n    if (!prefix || typeof prefix !== 'string') continue;\n\n    if (prefix[prefix.length - 1] !== '/') {\n      prefix = prefix + '/';\n      rules[i].prefix = prefix;\n    }\n\n    var segments = prefix.split('/');\n    if (segments.length !== 3 || !segments[0] || !segments[1] || segments[3]) continue;\n\n    if (Array.isArray(rules[i].terms)) {\n      map[prefix] = rules[i];\n    }\n  }\n\n  var keys = Object.keys(map);\n  var filtered = new Array(keys.length);\n\n  for (i = 0, l = keys.length; i < l; ++i) {\n    filtered[i] = map[keys[i]];\n  }\n\n  return filtered;\n}","map":{"version":3,"sources":["/Users/adamsarli/Coding/my-portfolio/node_modules/newrelic/lib/metrics/normalizer/tx_segment.js"],"names":["logger","require","child","component","module","exports","TxSegmentNormalizer","terms","prototype","normalize","path","currentTerm","prefix","i","length","lastIndexOf","fragment","slice","parts","split","result","prev","segment","j","indexOf","push","trace","matched","ignore","value","join","load","json","Array","isArray","filterRules","warn","rules","map","Object","create","l","segments","keys","filtered"],"mappings":"AAAA;;AAEA,IAAIA,MAAM,GAAGC,OAAO,CAAC,cAAD,CAAP,CAAwBC,KAAxB,CAA8B;AAACC,EAAAA,SAAS,EAAE;AAAZ,CAA9B,CAAb;;AAEAC,MAAM,CAACC,OAAP,GAAiBC,mBAAjB;;AAEA,SAASA,mBAAT,GAA+B;AAC7B,OAAKC,KAAL,GAAa,EAAb;AACD;AAED;;;;;;;;;;;;;;;;;;;;;AAmBAD,mBAAmB,CAACE,SAApB,CAA8BC,SAA9B,GAA0C,SAASA,SAAT,CAAmBC,IAAnB,EAAyB;AACjE,MAAIC,WAAJ;AACA,MAAIC,MAAJ;;AACA,OAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAAKN,KAAL,CAAWO,MAA/B,EAAuCD,CAAC,EAAxC,EAA4C;AAC1CF,IAAAA,WAAW,GAAG,KAAKJ,KAAL,CAAWM,CAAX,CAAd;AACAD,IAAAA,MAAM,GAAGD,WAAW,CAACC,MAArB;;AACA,QAAIF,IAAI,CAACK,WAAL,CAAiBH,MAAjB,EAAyB,CAAzB,MAAgC,CAAC,CAArC,EAAwC;AACtC;AACD;;AACD,QAAII,QAAQ,GAAGN,IAAI,CAACO,KAAL,CAAWL,MAAM,CAACE,MAAlB,CAAf;AACA,QAAII,KAAK,GAAGF,QAAQ,CAACG,KAAT,CAAe,GAAf,CAAZ;AACA,QAAIC,MAAM,GAAG,EAAb;AACA,QAAIC,IAAJ;AAEA,QAAIC,OAAJ;;AACA,SAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGL,KAAK,CAACJ,MAA1B,EAAkCS,CAAC,EAAnC,EAAuC;AACrCD,MAAAA,OAAO,GAAGJ,KAAK,CAACK,CAAD,CAAf;AAEA,UAAID,OAAO,KAAK,EAAZ,IAAkBC,CAAC,GAAG,CAAJ,KAAUL,KAAK,CAACJ,MAAtC,EAA8C;;AAE9C,UAAIH,WAAW,CAACJ,KAAZ,CAAkBiB,OAAlB,CAA0BF,OAA1B,MAAuC,CAAC,CAA5C,EAA+C;AAC7C,YAAID,IAAI,KAAK,GAAb,EAAkB;AAClBD,QAAAA,MAAM,CAACK,IAAP,CAAYJ,IAAI,GAAG,GAAnB;AACD,OAHD,MAGO;AACLD,QAAAA,MAAM,CAACK,IAAP,CAAYJ,IAAI,GAAGC,OAAnB;AACD;AACF;;AACDtB,IAAAA,MAAM,CAAC0B,KAAP,CAAa,oCAAb,EAAmDhB,IAAnD,EAAyDC,WAAzD;AACA,WAAO;AACLgB,MAAAA,OAAO,EAAE,IADJ;AACU;AACfC,MAAAA,MAAM,EAAE,KAFH;AAEU;AACfC,MAAAA,KAAK,EAAEjB,MAAM,GAAGQ,MAAM,CAACU,IAAP,CAAY,GAAZ;AAHX,KAAP;AAKD;;AAED,SAAO;AACLH,IAAAA,OAAO,EAAE,KADJ;AACW;AAChBC,IAAAA,MAAM,EAAE,KAFH;AAEW;AAChBC,IAAAA,KAAK,EAAEnB;AAHF,GAAP;AAKD,CAxCD;;AA0CAJ,mBAAmB,CAACE,SAApB,CAA8BuB,IAA9B,GAAqC,SAASA,IAAT,CAAcC,IAAd,EAAoB;AACvD,MAAIC,KAAK,CAACC,OAAN,CAAcF,IAAd,CAAJ,EAAyB;AACvB,SAAKzB,KAAL,GAAa4B,WAAW,CAACH,IAAD,CAAxB;AACD,GAFD,MAEO;AACLhC,IAAAA,MAAM,CAACoC,IAAP,CACE,yDADF,EAEE,OAAOJ,IAFT,EAGEA,IAHF;AAKD;AACF,CAVD;;AAYA,SAASG,WAAT,CAAqBE,KAArB,EAA4B;AAC1B,MAAIC,GAAG,GAAGC,MAAM,CAACC,MAAP,CAAc,IAAd,CAAV;;AAEA,OAAK,IAAI3B,CAAC,GAAG,CAAR,EAAW4B,CAAC,GAAGJ,KAAK,CAACvB,MAA1B,EAAkCD,CAAC,GAAG4B,CAAtC,EAAyC,EAAE5B,CAA3C,EAA8C;AAC5C,QAAID,MAAM,GAAGyB,KAAK,CAACxB,CAAD,CAAL,CAASD,MAAtB;AAEA,QAAI,CAACA,MAAD,IAAW,OAAOA,MAAP,KAAkB,QAAjC,EAA2C;;AAE3C,QAAIA,MAAM,CAACA,MAAM,CAACE,MAAP,GAAgB,CAAjB,CAAN,KAA8B,GAAlC,EAAuC;AACrCF,MAAAA,MAAM,GAAGA,MAAM,GAAG,GAAlB;AACAyB,MAAAA,KAAK,CAACxB,CAAD,CAAL,CAASD,MAAT,GAAkBA,MAAlB;AACD;;AAED,QAAI8B,QAAQ,GAAG9B,MAAM,CAACO,KAAP,CAAa,GAAb,CAAf;AACA,QAAIuB,QAAQ,CAAC5B,MAAT,KAAoB,CAApB,IAAyB,CAAC4B,QAAQ,CAAC,CAAD,CAAlC,IAAyC,CAACA,QAAQ,CAAC,CAAD,CAAlD,IAAyDA,QAAQ,CAAC,CAAD,CAArE,EAA0E;;AAE1E,QAAIT,KAAK,CAACC,OAAN,CAAcG,KAAK,CAACxB,CAAD,CAAL,CAASN,KAAvB,CAAJ,EAAmC;AACjC+B,MAAAA,GAAG,CAAC1B,MAAD,CAAH,GAAcyB,KAAK,CAACxB,CAAD,CAAnB;AACD;AACF;;AAED,MAAI8B,IAAI,GAAGJ,MAAM,CAACI,IAAP,CAAYL,GAAZ,CAAX;AACA,MAAIM,QAAQ,GAAG,IAAIX,KAAJ,CAAUU,IAAI,CAAC7B,MAAf,CAAf;;AAEA,OAAKD,CAAC,GAAG,CAAJ,EAAO4B,CAAC,GAAGE,IAAI,CAAC7B,MAArB,EAA6BD,CAAC,GAAG4B,CAAjC,EAAoC,EAAE5B,CAAtC,EAAyC;AACvC+B,IAAAA,QAAQ,CAAC/B,CAAD,CAAR,GAAcyB,GAAG,CAACK,IAAI,CAAC9B,CAAD,CAAL,CAAjB;AACD;;AAED,SAAO+B,QAAP;AACD","sourcesContent":["'use strict'\n\nvar logger = require('../../logger').child({component: 'tx_segment_normalizer'})\n\nmodule.exports = TxSegmentNormalizer\n\nfunction TxSegmentNormalizer() {\n  this.terms = []\n}\n\n/**\n * This normalize method is wicked. The best bet is to read the spec:\n * https://newrelic.atlassian.net/wiki/pages/viewpage.action?spaceKey=eng&title=Language+agent+transaction+segment+terms+rules\n *\n * A copy paste of the rules that were followed:\n *  1. Find the first rule where the prefix key matches the prefix of the\n *     transaction name. If no matching rules are found, abort.\n *  2. Strip the prefix from the transaction name.\n *  3. Split the rest of the transaction name into segments on slashes ('/').\n *  4. For each segment:\n *      If the segment appears in the array of strings given under the terms key,\n *      keep it unchanged. Else, replace it with a placeholder ('*')\n *  5. Collapse all adjacent placeholder segments into a single '*' segment.\n *  6. Join together the modified segments with slashes, and re-prepend the prefix.\n *\n * @param {string} path - The transaction metric path to normalize.\n *\n * @return {NormalizationResults} - The results of normalizing the given path.\n */\nTxSegmentNormalizer.prototype.normalize = function normalize(path) {\n  var currentTerm\n  var prefix\n  for (var i = 0; i < this.terms.length; i++) {\n    currentTerm = this.terms[i]\n    prefix = currentTerm.prefix\n    if (path.lastIndexOf(prefix, 0) === -1) {\n      continue\n    }\n    var fragment = path.slice(prefix.length)\n    var parts = fragment.split('/')\n    var result = []\n    var prev\n\n    var segment\n    for (var j = 0; j < parts.length; j++) {\n      segment = parts[j]\n\n      if (segment === '' && j + 1 === parts.length) break\n\n      if (currentTerm.terms.indexOf(segment) === -1) {\n        if (prev === '*') continue\n        result.push(prev = '*')\n      } else {\n        result.push(prev = segment)\n      }\n    }\n    logger.trace('Normalizing %s because of rule: %s', path, currentTerm)\n    return {\n      matched: true, // To match MetricNormalizer\n      ignore: false, // ^^\n      value: prefix + result.join('/')\n    }\n  }\n\n  return {\n    matched: false, // To match MetricNormalizer\n    ignore: false,  // ^^\n    value: path\n  }\n}\n\nTxSegmentNormalizer.prototype.load = function load(json) {\n  if (Array.isArray(json)) {\n    this.terms = filterRules(json)\n  } else {\n    logger.warn(\n      'transaction_segment_terms was not an array got: %s (%s)',\n      typeof json,\n      json\n    )\n  }\n}\n\nfunction filterRules(rules) {\n  var map = Object.create(null)\n\n  for (var i = 0, l = rules.length; i < l; ++i) {\n    var prefix = rules[i].prefix\n\n    if (!prefix || typeof prefix !== 'string') continue\n\n    if (prefix[prefix.length - 1] !== '/') {\n      prefix = prefix + '/'\n      rules[i].prefix = prefix\n    }\n\n    var segments = prefix.split('/')\n    if (segments.length !== 3 || !segments[0] || !segments[1] || segments[3]) continue\n\n    if (Array.isArray(rules[i].terms)) {\n      map[prefix] = rules[i]\n    }\n  }\n\n  var keys = Object.keys(map)\n  var filtered = new Array(keys.length)\n\n  for (i = 0, l = keys.length; i < l; ++i) {\n    filtered[i] = map[keys[i]]\n  }\n\n  return filtered\n}\n"]},"metadata":{},"sourceType":"script"}